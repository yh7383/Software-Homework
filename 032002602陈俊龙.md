**Github仓库链接：**[Digoler/032002602 (github.com)](https://github.com/Digoler/032002602)

# 一、PSP表格

| **PSP2.1**                               | **Personal Software             Process Stages** | 预计耗时 （分钟） | 实际耗时（分钟） |
| ---------------------------------------- | ------------------------------------------------ | ----------------- | ---------------- |
| Planning                                 | 计划                                             | 20                | 30               |
| · Estimate                               | · 估计这个任务需要多少时间                       | 5                 | 14               |
| Development                              | 开发                                             | 950               | 1145             |
| · Analysis                               | · 需求分析 (包括学习新技术)                      | 200               | 500              |
| · Design Spec                            | · 生成设计文档                                   | 40                | 20               |
| · Design Review                          | · 设计复审                                       | 20                | 10               |
| · Coding Standard                        | · 代码规范 (为目前的开发制定合适的规范)          | 30                | 37               |
| · Design                                 | · 具体设计                                       | 120               | 130              |
| · Coding                                 | · 具体编码                                       | 900               | 1600             |
| · Code Review                            | · 代码复审                                       | 180               | 210              |
| · Test                                   | · 测试（自我测试，修改代码，提交修改）           | 180               | 150              |
| Reporting                                | 报告                                             | 45                | 50               |
| · Test Repor                             | · 测试报告                                       | 30                | 70               |
| · Size Measurement                       | · 计算工作量                                     | 20                | 15               |
| ·  Postmortem & Process Improvement Plan | · 事后总结, 并提出过程改进计划                   | 15                | 20               |
|                                          | · 合计                                           | 2755              | 4001             |

# 二、任务要求的实现

## 2.1 项目与技术栈

<img src="https://img-blog.csdnimg.cn/43953478bd6349e0a8a8fc7a2263a413.png#pic_center" alt="image-20220915092845180" style="zoom: 80%;" />

### 2.1.1任务分解

1. **项目流程分析：**
   + 进入要爬取的网页进行网页结构的分析（网页的布局），分析要用到的爬虫技术；
   + 如何从爬取的内容中提取出我们想要的数据
   + 如何将我们数据存储到Excel中
   + 如何对存储的数据进行每日热点分析
   + 如何对存储的数据进行数据可视化（生成图表、制作可视化大屏）

2. **各个流程实现：**

   - 网页内容爬取：Python中的pyppeteer模块、os模块、xpath数据解析、异步爬虫
   - 文本中提取数据：re模块、os模块
   - 数据存储到Excel中：openpyxl模块
   - 数据每日热点分析
   - 数据可视化：openpyxl模块、第三方库pyecharts

3. **技术栈**

   python（pyppeteer模块、asyncio模块、os模块、re模块、openpyxl模块、第三方库pyecharts、多线程的使用）

## 2.2 爬虫与数据处理

### 2.2.1 爬虫

1. **需求分析：**统计中国大陆每日本土新增确诊人数以及新增新增无症状感染人数（大陆新增+精确到省份+港澳台相关数据），而我们要的数据在以下的文本内容文本中都可以得到，因此我们要想办法得到下面的文本。

<img src="https://img-blog.csdnimg.cn/b40550853b644a47beee4b18fc548450.png#pic_center" alt="image-20220915121225211" style="zoom:50%;" />

2. **实现过程：**

   **实现思路**：![image-20220915131001952](https://img-blog.csdnimg.cn/49921bfa49964694af1c0f42d872214c.png#pic_center)

   **需要导入的模块和库：**

   ```python
   import asyncio
   from pyppeteer import launch
   import os
   from lxml import etree
   import random
   import time
   ```
   
   **（1）获取到每一天的url**

- ​	**获取每一页的url**

  - 思路：观察几个页面对应的url，它是有规律的，我们只需要通过循环生成就可以。

  ![image-20220915131902485](https://img-blog.csdnimg.cn/5aa4373860a84d33b1b9bc5e4e6bbbba.png#pic_center)

  ![image-20220915131920141](https://img-blog.csdnimg.cn/fb58049453c842f487d2eb9907e30033.png#pic_center)

  ![image-20220915131930939](https://img-blog.csdnimg.cn/269e241615c74739acad2d09704aa613.png#pic_center)

  - 代码：

    ```python
    # 获取每一页面的url(共42页)
    def get_page_url() -> list:
        url_list = []  # 用来保存42个页面的42个url
        for page in range(1, 42):
            if page == 1:   # 第一页的时候，页面的url比较特殊，我们单独拿出来保存
                url_list.append('http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml')
            else:   # 剩下的页面的url有规律，我们直接用 lambda匿名函数生成即可
                # 拼接成完整的url
                url_list.append(lambda page: 'http://www.nhc.gov.cn/xcs/yqtb/list_gzbd_' + str(page) + '.shtml')
        return url_list
    ```

生成每个页面对应的url后，我们只需要遍历每个页面，到每个页面的页面源代码中收集到每一天的页面对应url即可。

因此我们需要获取每个页面的源代码。但是这个网页有反爬机制（加密机制为五代瑞数，可以生成动态Cookie）,当我们频繁访问的时候，它会没有相应，并且返回状态码412。加密机制有点复杂，逆向爬虫难度比较大，因此这里我使用的`pyppeteer`模块。pyppeteer是一个自动化测试工具，可以看作是Selenium的升级版。具体内容的介绍可以到：[静觅丨崔庆才的个人站点 - Python 爬虫教程 (cuiqingcai.com)](https://cuiqingcai.com/)这个网站去查看。

- **获取网页源代码函数设计(关键函数)**

  - 代码

    ```python
    # 获取网页源代码
    async def FetchUrl(url: str) -> str:
        # launch 方法新建一个 Browser 对象，然后赋值给 browser
        browser = await launch({'headless': True, 'dumpio': True, 'autoClose': True})
        # 调用 newPage方法相当于浏览器中新建了一个选项卡，同时新建了一个Page对象。
        page = await browser.newPage()
        # 绕过浏览器检测（关键步骤）
        await page.evaluateOnNewDocument('() =>{ Object.defineProperties(navigator,'
                                         '{ webdriver:{ get: () => false } }) }')
        # 设置 UserAgent
        await page.setUserAgent(
            'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Mobile Safari/537.36 Edg/105.0.1343.27')
        # Page 对象调用了 goto 方法就相当于在浏览器中输入了这个 URL，浏览器跳转到了对应的页面进行加载
        await page.goto(url)
        await asyncio.wait([page.waitForNavigation()], timeout=3)  # 等待页面加载完成
        # 页面加载完成之后再调用 content 方法，返回当前浏览器页面的源代码
        page_content = await page.content()
        
        await browser.close()
        return page_content
    
    
    # 获取页面源代码
    def get_page_source(text_url: str) -> str:
        # 创建事件循环，并将协程注册到事件循环中
        return asyncio.get_event_loop().run_until_complete(FetchUrl(text_url))
    ```

  - 代码解释：

    - ​	pyppeteer运用了协程的一些知识相关内容可以参考：[【2022 年】Python3 爬虫教程 - 协程的基本原理 | 静觅 (cuiqingcai.com)](https://cuiqingcai.com/202271.html)、[(93条消息) 轻松理解 Python 中的 async await 概念_Likianta Me的博客-CSDN博客_async await python](https://blog.csdn.net/Likianta/article/details/90123678)
    - pyppeteer是一种自动化测试工具，现在已经被广泛用于网络爬虫中来应对 JavaScript 渲染的页面的抓取，Pyppeteer 是依赖于 Chromium 这个浏览器来运行的。有了 Pyppeteer 之后，我们就可以免去那些繁琐的环境配置等问题。如果第一次运行的时候，Chromium 浏览器没有安全，那么程序会帮我们自动安装和配置，就免去了繁琐的环境配置等工作。另外 Pyppeteer 是基于 Python 的新特性 async 实现的，所以它的一些执行也支持异步操作，性能比较强大。
    - pyppeteer是基于async实现的，由asynic定义的函数称为协程对象，协程对象不能够直接运行，有一种运行的方法是将它放到事件循环当中，在`get_page_source(text_url: str) -> str:`中,`asyncio.get_event_loop()`表示创建了一个事件循环，`asyncio.get_event_loop().run_until_complete(FetchUrl(text_url))`后面的`run_until_complete`表示将`FetchUrl`这个协程对象注册到事件循环中。（当然我们可以将多个任务加入到事件循环当中，实现多任务协程）
    - `await`（挂起）的简单理解，就是凡是协程对象，前面就要加一个await，而pyppeteer中的方法的返回值都是协程对象。（当我们在进行多任务协程时，当一个协程任务遇到`await`挂起时，事件循环就会先去执行其他没有被挂起的协程对象，充分利用资源）

- **从页面的网络源代码中获取每日的url**

  - 代码：

    ```python
    # 获取每一个页面(目前共42页)的24条url
    def get_link_url(html_txt: str) -> list:
        day_urls = []  # 定义一个用来存储所有页面内24条url的列表
        html_tree = etree.HTML(html_txt)    # 生成etree对象
        li_list = html_tree.xpath("/html/body/div[3]/div[2]/ul/li")  # 提取出页面源代码中的所有li对象
        for li in li_list:
            day_url = li.xpath('./a/@href')  # 对每个li对象，分别提取出它里面的 a 标签的属性值
            day_url = "http://www.nhc.gov.cn" + "".join(day_url[0])  # 由于提取到的href是相对于网页的相对路径，我们需要拼接成完整的url
            day_urls.append(day_url)
        return day_urls
    ```

**（2）获取每日发布的内容并保存为txt文件**

- **从每天的页面中获取文本**（传入的参数是每一天的页面对应的源代码）

  - 代码：

    ```python
    #  获取每一天的发布内容
    def get_each_day_content(html_txt: str) -> str:
        html_tree = etree.HTML(html_txt)
        text = html_tree.xpath('/html/body/div[3]/div[2]/div[3]//text()')   # 定位到文本所在的位置，进行提取
        day_text = "".join(text)    # xpath返回的是一个列表，我们需要把它拼接成字符串
        return day_text
    ```

- **将文本保存为txt文件**

  - 代码：

    ```python
    # 保存文件到指定的目录
    def save_file(path: str, filename: str, text: str):
        if not os.path.exists(path):
            os.makedirs(path)
        # 保存文件
        with open(path + filename + ".txt", 'w', encoding='utf-8') as f:
            f.write(text)
    ```

- **主函数的实现：**

  - 代码：

    ```python
    if "__main__" == __name__:
        start = time.time()  # 设置时间戳，查看程序运行时间
        page_urls = get_page_url()    # 获取42个页面的url
        for url in page_urls:   # 遍历每一个疫情通报页面
            page_text = get_page_source(url)    # 这一步是获取疫情通报页面的页面源代码
            time.sleep(random.randint(3, 6))    # 设计一个随机的程序睡眠时间，让程序更好地模仿人进行爬取页面数据
            filenames = get_filenames(page_text)    # 提取出当前疫情通报页面的所有文件名
            index = 0   # 设置文件名索引
            links = get_link_url(page_text)  # 获取当前疫情通报页面的24条url
            # ----------获取这个页面24天的疫情通报内容文本----------
            for link in links:
                html = get_page_source(link)    # 获取这一天的页面源代码
                content = get_each_day_content(html)    # 从页面源代码中提取出我们需要的文本
                print(filenames[index] + "爬取成功")    # 程序运行情况提示
                save_file('C:/Users/ASUS/Desktop/数据防控数据1/', filenames[index], content)  # 保存文件到指定文件夹
                index = index + 1
            # ---------------------------------------- --------
            print("-----" * 20)  # 爬取完一页所设置的标识情况，方便观察程序运行情况
        end = time.time()
        print(f"所有内容爬取完成，所用时间：{end - start}s")
    ```

（3）**代码优化：使用线程池**

- 改变主函数的结构：爬取九百多条平均只要10分钟。

  - 代码：

    ```python
    # 定义线程池对象
    def fetch_task(url):
        page_text = get_page_source(url) # 这一步是获取疫情通报页面的页面源代码
        filenames = get_filenames(page_text) # 提取出当前疫情通报页面的所有文件名
        index = 0# 设置文件名索引
        links = get_link_url(page_text)# 获取当前疫情通报页面的24条url
        for link in links:
            html = get_page_source(link)# 获取这一天的页面源代码
            content = get_each_day_content(html)# 从页面源代码中提取出我们需要的文本
            print(filenames[index] + "爬取成功")# 程序运行情况提示
            save_file(f'C:/Users/ASUS/Desktop/数据防控数据1/', filenames[index], content) # 保存文件到指定文件夹
            index = index + 1
    
    
    def main():
        start = time.time()	# 设置时间戳
        urls = get_page_url() # 获取42个页面的url
        pool = Pool(multiprocessing.cpu_count())  # 创建线程池，根据CPU能力切换线程
        pool.map(fetch_task, urls)  # 将任务加入线程池
    
        end = time.time()
        print(end - start)
    
    
    if "__main__" == __name__:
        main()
    ```

    

3. **运行结果：**

   ![image-20220916110517237](https://img-blog.csdnimg.cn/729bc96561ac460580ec14c208db968b.png#pic_center)

   <img src="https://img-blog.csdnimg.cn/8698d16742dd41618d4a3daeb1741906.png#pic_center" alt="image-20220917161439207" style="zoom:80%;" />
   
   

### 2.2.2  数据处理

1. **需求分析：**（1）统计**中国大陆**每日本土**新增确诊**人数及**新增无症状感染**人数，境外输入类型和疑似病例等无需统计。

   ​                   （2）统计**所有省份**包括港澳台每日本土**新增确诊**人数及**新增无症状感染**人数，境外输入类型和疑似病例等无需统计。

   要从文本中提取数据，需要用到正则表达式，我使用了openpyxl模块将数据存储到excel中。
   
2. **实现过程：**

- **需要导入的模块：**

```python
import re
import os
from openpyxl.chart import (
    Reference,
    Series,
    BarChart,
    label
)
from openpyxl import Workbook
import time
```

- **定义一个全局变量（名称列表）**

  ```python
  province_name = ['福建', '河北', '河南', '湖南', '湖北', '广东', '北京', '上海', '天津', '重庆', '四川', '黑龙江', '内蒙古', '西藏', '宁夏', '浙江', '山东','安徽', '江西', '山西', '吉林', '江苏', '云南', '贵州', '陕西', '甘肃', '青海', '辽宁', '广西', '新疆', '海南']
  ```

- **获取本土新增病例并按省份进行统计**

  - 代码实现：

    ```python
    # 获取本土病例相关数据
    def get_Local_data(content: str) -> dict:
        area_num = {'本土病例': 0}
        information = verify_local_info(content)  # 提取于本土病例相关内容
        if information == 0:  # 如果没有与新增本土病例相关的内容，直接返回
            area_num = refine_dict(area_num)    # 将所有省份数据设为0
            return area_num
        all_increase = (re.search(r'\d+', information)).group()  # all_increase 为本土新增的病例数
        area_num['本土病例'] = all_increase     # 本土新增病例的值为 all_increase
        # ----------------- 提取省份数据-------------------------
        # 获取出每个省份新增人数的文本信息
        province_info = (re.search(r'（.*?）', information)).group()
        # 解决文本格式不一致的问题和各个省份数据提取（对字符串进行切割）,比如说xx省x例，其中xx市几例；xx省x例，其中xx市x例
        if '；' in province_info:
            increase_list = province_info.split('；')
        else:
            increase_list = province_info.split('，')
        # 遍历切割完的字符串（即包含各个省份数据的列表，对其进行正则提取）
        for area_increase in increase_list:
            # 提取出省份的相关信息
            area_name = re.search(r'[\u4e00-\u9fa5]+', area_increase).group()
            # 避免一些不规范文本的出现，将省份数据格式化
            format_name(area_increase, area_name, area_num, all_increase)
        area_num = refine_dict(area_num)    # 将无新增病例的省份数据初始化为0
        return area_num
    ```

  - **调用函数1：**用于检测文本中是否存在与本土病例有关的内容

    - 代码：

      ```python
      def verify_local_info(content: str) -> str:
          if re.search(r"[^\u4e00-\u9fa5]本土病例.*。", content) is None:
              return 0
          else:
              info = (re.search(r"[^\u4e00-\u9fa5]本土病例.*。", content)).group()
              return info
      ```

  - **调用函数2：**将无新增病例或无新增无症状感染者的省份的数据初始化为0
    
    - 代码：

  ```python
  # 改进字典，将没有新增病例或无症状感染者的省份的值设为0
  def refine_dict(dictionary):
      for province in province_name:
          if province not in dictionary:
              dictionary[province] = 0
      return dictionary
  ```

  - **调用函数3：**解决文本格式不统一的问题（均在福建或在福建），并格式化省份的名称

    - 代码：

    ```python
    # 用于解决出现类似：本土新增x例（均在福建或在福建）的问题
    def format_name(content: str, area_name: str, diction: dict, num):
        # 如果提取出的地区名称不在省份名称列表中，则需要格式化
        if area_name not in province_name:
            area_name = verify_name(area_name)  # 将省份名称格式化
        if re.search(r'[0-9]+', content) is None or content[1] == '均' or content[1] == '在':
            diction[area_name] = num
        else:
            # 提取出省份的新增数量
            increase_num = re.search(r'[0-9]+', content).group()
            diction[area_name] = increase_num
    ```

  - **调用函数4：**格式化省份名称的函数

    - 代码：

      ```python
      # 格式化省份名称的实现
      def verify_name(name_str: str) -> str:
          for name in province_name:
              if name in name_str:
                  return name
      ```

- **提取新增无症状感染者数据：**

  - 代码：

    ```python
    # 获取新增无症状感染者数据
    def get_asymptomatic_infected_data(content):
        asymptomatic_dict = {'新增无症状感染者': 0}  # 定义一个字典来存储无症状新增数据
        if re.search(r'本土[0-9]+例（.*?）', content) is None:   # 如果正则获取不到我们需要的文本信息，直接返回
            return asymptomatic_dict
        else:
            # 获取本土新增无症状的文本信息
            mainland_info = re.search(r'本土[0-9]+例（.*?）', content).group()
            # 获取本土新增无症状的数量
            asymptomatic_increase = re.search(r'[0-9]+', mainland_info).group()
            asymptomatic_dict['新增无症状感染者'] = asymptomatic_increase
            # 正则提取出与省份有关的文本信息
            province_asymptomatic_increase = re.search(r'（.*?）', mainland_info).group()
            # 对省份文本信息进行切割
            if len(province_asymptomatic_increase.split('；')) == 1:
                province_list = province_asymptomatic_increase.split('，')
            else:
                province_list = province_asymptomatic_increase.split('；')
            # 遍历切割后形成的列表
            for province in province_list:
                area_name = re.search(r'[\u4e00-\u9fa5]+', province).group()    # 提取出含省份数据的文本
                # 处理特殊格式，并格式化省份名称
                format_name(province, area_name, asymptomatic_dict, asymptomatic_increase)
            asymptomatic_dict = refine_dict(asymptomatic_dict)  # 将无新增无症状感染者的省份数据初始化为0
            return asymptomatic_dict
    ```
    
  - 代码解释：在`get_asymptomatic_infected_data(content)`函数中，调用了`format_name`函数和`refine_dict`函数，在`refine_dict`函数中又调用了`verify_name`函数

- **将数据保存在Excel中**：

  - 代码：

    ```python
    
    # 将数据保存至excel中
    def save_to_excel(data_1: dict, data_2: dict, filename):
        wb = Workbook()  # 新建一个Workbook对象
        wb.remove(wb.active)    # 删除当前Workbook自带的sheet
        worksheet_1 = wb.create_sheet("新增本土病例")    # 在工作簿中新建一个sheet，用来保存本土病例数据
        worksheet_2 = wb.create_sheet("新增无症状感染者")   # 新建一个sheet，保存新增无症状数据
        # 以下是优化后的Excel写入函数，在进行大量文件处理时用得到。
        excel_write(worksheet_1, data_1)    # 在sheet中写入本土病例数据
        excel_write(worksheet_2, data_2)    # 在另一个sheet中写入数据新增无症状数据
        wb.save(f'C:/Users/ASUS/Desktop/疫情防控数据excel1/{filename}.xlsx')  # 保存为Excel
    ```

  - 其中**Excel_write()函数**如下：

    - 代码：

      ```python
      # 定义一个Excel写入函数，用于大量数据的处理。
      def excel_write(worksheet, data):
          name = list(data.keys())
          value = list(data.values())
          long = len(name)
          # 开始写入Excel
          for row in range(1, long + 1):
              worksheet.cell(row=row, column=1, value=name[row-1])
              worksheet.cell(row=row, column=2, value=int(value[row-1]))
      ```

- **主函数main()实现**：

  - 代码：

    ```python
    if __name__ == '__main__':
        start = time.time()  # 设置时间戳，查看程序运行时间
        path = 'C:/Users/ASUS/Desktop/疫情防控数据'
        dirs = os.listdir(path)	# 获取所有的文件
        dirs.reverse()	# 将文件倒置，从最新的文件开始提取数据
        for file in dirs:
          	# 数据只提取到2021-05-15，因为再早之前的文件格式比较乱。
            if file == '2021-05-15-截至5月14日24时新型冠状病毒肺炎疫情最新情况.txt':
                break
            f = open(file=f'C:/Users/ASUS/Desktop/疫情防控数据/{file}', mode='r', encoding='utf-8')
            text = f.read()
            # 获取新增本土病例数据
            local_data = get_Local_data(text)
            # 获取新增无症状感染者数据
            asymptomatic_data = get_asymptomatic_infected_data(text)
            # 保存文件
            save_to_excel(local_data, asymptomatic_data, file)
            f.close()
            print(f"{file}提取完成")	# 代码运行提示
        end = time.time()
        print(f"所有文件提取完成，所用时间：{end - start}s")
    ```

- 运行结果：![image-20220916110345374](https://img-blog.csdnimg.cn/c10049a868264a2482f2c33c2a42551e.png#pic_center)

## 2.3 数据统计接口部分的性能改进

### 2.3.1 数据统计接口功能

用户可以根据需要获取疫情数据，功能分为三种：

（1）获取所有的疫情数据

（2）获取当天最新疫情数据

（3）生成可视化大屏，可以根据需要输入指定日期，会自动弹出数据可视化大屏

![image-20220917155850958](https://img-blog.csdnimg.cn/46f1ba1c01824217b96b133d85965d3e.png#pic_center)

### 2.3.2 数据统计接口性能分析

**一、功能（1）运行时间：**

原本运行时间是2个小时左右，我在主函数中设置了sleep(3)，后面经过调试发现，这3秒取消掉并不会造成造成请求过于频繁导致请求无响应，取消后，发现总体的运行时间还是受`FetchUrl`函数影响，想利用多任务协程提高速度，但是引入多任务协程后，pyppetteer请求的响应内容不全，因此失效，最多引入了多线程来爬取全部内容，平均时间只需要10分钟左右。

**二、功能（2）运行时间：**

![image-20220917192543719](https://img-blog.csdnimg.cn/2354ea0118ab45a3be866e0f07a00cf9.png#pic_center)

| 函数名               | 所用时间（单位：秒）  |
| -------------------- | --------------------- |
| FetchUrl第一次调用   | 4.218430042266846     |
| get_today_url        | 0.0                   |
| FetchUrl第二次调用   | 0.4.535336971282959   |
| get_each_day_content | 0.0009996891021728516 |
| save_file            | 0.0                   |
| draw_today_data      | 0.04435372352600098   |

通过上表可以发现，在功能实现过程中，花费时间最长的是`FetchUrl`函数的调用，每次调用都要4秒多，但经过多次的尝试，在pyppeteer使用过程中，已经没有办法能够让FetchUrl函数的执行时间更快。

- 性能分析图

  ![image-20220917222237979](https://img-blog.csdnimg.cn/b64af06342cd4659b7f0055d642571e6.png#pic_center)

  ![image-20220917230900440](C:\Users\ASUS\Desktop\我的笔记\软工实践\Demo01.assets\image-20220917230900440.png)

  

**三、功能（3）运行时间：**

![image-20220917195343525](https://img-blog.csdnimg.cn/2e263f4afcbd40d8ad08f94de837c102.png#pic_center)

通过上图可以发现，功能（3）实现过程中速度较快，没有可以进一步改进的地方。

- 性能分析图

<img src="https://img-blog.csdnimg.cn/bd0767c3bfed49acbae257f8f509ae41.png#pic_center" alt="image-20220917223538147" style="zoom:80%;" />

**四、耗时最长的函数**

```python
# 获取网页源代码
async def FetchUrl(url: str) -> str:
    # launch 方法新建一个 Browser 对象，然后赋值给 browser
    browser = await launch({'headless': True, 'dumpio': True, 'autoClose': True})
    # 调用 newPage方法相当于浏览器中新建了一个选项卡，同时新建了一个Page对象。
    page = await browser.newPage()
    # 绕过浏览器检测（关键步骤）
    await page.evaluateOnNewDocument('() =>{ Object.defineProperties(navigator,'
                                     '{ webdriver:{ get: () => false } }) }')
    # 设置 UserAgent
    await page.setUserAgent(
        'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Mobile Safari/537.36 Edg/105.0.1343.27')
    # Page 对象调用了 goto 方法就相当于在浏览器中输入了这个 URL，浏览器跳转到了对应的页面进行加载
    await page.goto(url)
    await asyncio.wait([page.waitForNavigation()], timeout=3)  # 等待页面加载完成
    # 页面加载完成之后再调用 content 方法，返回当前浏览器页面的源代码
    page_content = await page.content()
    
    await browser.close()
    return page_content


# 获取页面源代码
def get_page_source(text_url: str) -> str:
    # 创建事件循环，并将协程注册到事件循环中
    return asyncio.get_event_loop().run_until_complete(FetchUrl(text_url))
```



## 2.4 每日热点的实现思路

**一、实现思路：**

   LSTM模型的一个常见用途是对长时间序列数据进行学习预测，例如得到了某商品前一年的日销量数据，我们可以用LSTM模型来预测未来一段时间内该商品的销量。对于疫情趋势的预测，我们也可以用这个模型来进行预测，进而提取出可能的每日热点。

**二、实现步骤：**总共分为五个步骤

数据预处理——> 构建数据集 ——> 训练LSTM模型 ——> 模型的泛化 ——> 做出预测结果的可视化

### 2.5.1 Excel数据可视化

**一、数据可视化实现：**

针对每一天单独生成一个Excel文件，一个Excel文件中包含两个sheet，分别保存每日新增本土病例数据和每日新增无症状感染者数据。（PS：港澳台相关数据在可视化大屏中展示）

1. 数据可视化界面组件：
   - 柱形图：横坐标显示省份，纵坐标显示各省份数据（每日为一个文件）
2. 设计思路：
   - 将所有需要的数据提取到Excel，再进行批量生成图表
3. 界面展示：

<img src="https://img-blog.csdnimg.cn/5eaf9c205b88431bae6c3a220a06b0f3.png#pic_center" alt="image-20220917160103402" style="zoom: 80%;" />

<img src="https://img-blog.csdnimg.cn/04fd0d8a61904641bf71e7504c0a0ceb.png#pic_center" alt="image-20220917160135547" style="zoom:80%;" />

### 2.5.2 数据可视化大屏

**一、数据可视化实现：**

利用pyecharts库进行图形绘制，通过地图一览全国疫情情况（本土确诊病例）；并根据输入的日期生成七天内全国各省新增确诊病例变化图；在本土新增病例Top3中列出了当天疫情情况最严重的前三个省份；港澳台数据通过柱状图展现，分为今日新增病例和累计病例。

1. 数据可视化界面组件：

   - 全国地图：显示各省份本土新增病例，一览疫情大致情况
   - 轮播图柱形图：显示七日内全国疫情数据
   - 柱形图排行榜：显示当日全国疫情较为严重的省份
   - 港澳台新增病例柱形图：显示当日香港、澳门、台湾新增数据
   - 港澳台累计确诊柱形图：显示到输入日期为止香港、澳门、台湾的累计确诊病例

2. 设计思路：

   - 按照爬取到的数据先设计出数据可视化地图（尚未调整样式）；

     ```python
     from pyecharts.charts import Map
     ```

   - 再根据七天行程卡的需要设计出一个含七日疫情情况的动态轮播图（尚未调整样式）；

     ```python
     from pyecharts.charts import Timeline, Bar
     ```

   - 根据爬取的港澳台数据分别提取出当日新增数据和累计确诊数据，使其在一个界面中显示；

     ```python
     from pyecharts.charts import Bar
     ```

   - 为了使大屏的内容展示更加紧密，再设计出一个排名榜来更充分地显示数据；

   - 最后利用`pyecharts`里面的`Page`将四个图表整合到一起，并调整显示样式；

3. 调用方法：

   在数据统计接口中输入3，再输入想要查询的日期即可。

4. 界面展示：

![未命名-副本(2)](https://img-blog.csdnimg.cn/d6282f1053644983ad650b78ad08fe04.gif#pic_center)



# 三、心得体会

## 3.1 爬虫心得体会

- 学习到的知识：

  `pyppeteer`的使用、自动化测试工具的使用、对`asyncion`模块、多线程、协程有了更加深刻的理解、复习了`aiohttp`、`aiofiles`模块，对网站的反爬机制有了更深刻的体会。

- 体会：

​		在爬取数据前，没有做好充足的规划，第一次爬取时之前用reques向浏览器发起请求，在请求返回源代码后直接用xpath对源代码数据解析进行数据的提取，爬取了网页前几天的数据后，发现返回的数据出现了一些格式的错误，修改完bug后继续进行爬取，结果变成xpath解析不到数据，观察了一下网页结构后，才发现网页的结构是不固定的，突然蹦出一个P 标签让xpath失去了功效，于是只能尝试用其他的办法。在改掉程序的结构后，第二次我重新爬取数据的时候，打算把整个文本数据爬取下来存为txt文件，后续再用正则表达式进行数据提取，还是直接使用requests发起网络请求，整个文本ue被顺利爬取下来，但是有的txt文件里面却是空的，于是我在爬取过程中打印requests.get()返回的`Response`的状态码，断断续续的返回412，关于状态码412，有许许多多的可能性，但是能断定网站有反爬机制，除了明显的生成的url是随机的以外，网站还使用了JS加密机制，放慢了requests的请求也是还会返回412。在经过了大量资料的搜索后，才发现了pyppeteer这个强大的工具（Selinum速度实在太慢了）。

​		在进行爬虫前一定要先做好测试，充分观察网页结构，确保方法可行后再进行爬取，爬取方法确定后，还要关注其他细节，比如存储为txt文件时文件名是否唯一的问题（不唯一会导致文件被覆盖）。基本程序结构确定后，还要继续优化爬取速度，比如使用多线程、多任务协程（异步请求、异步文件存储）等

## 3.2 数据统计心得体会

- 学习到的知识：openpyxl模块的使用、正则表达式的熟练使用、批量提取数据能力的提升。
- 体会：

​	之前一定很畏惧使用正则表达式，在经过本次作业后，被正则表达式的强大功能震撼到了，但是这个网站发布内容的格式都不太统一，要完全正确提取的难度有点大，有种面向程序结果编程的感觉。openpyxl模块也是本次作业才学习，这个模块能够实现办公自动化，比较容易上手，功能也非常强大，不过学习本模块没有看官方文档，在网络上杂七杂八的搜索资料，才做出一张比较可观的柱形图。

​	利用正则提取数据时，需要注意设计存储数据的数据结构，注意与后续需求相呼应，不然处理起来会很麻烦。

## 3.3 数据可视化心得体会

- 学到的知识：pyecharts库的使用，利用工具进行数据处理的能力有所提高。
- 体会：

​		为了做出数据可视化大屏，我先是了解了一下Echart，发现它好像是基于Javascript的，而自身对JS又不是特别熟练，于是通过搜索资料了解到了Pyecharts这个库，但是B站上面都没有相关的教学视频，最后只能通过看官方文档一步步学习，前前后后大概学了两天用起来才逐渐熟练，之前对于官方文档都有一种畏惧感，通过这次作业，终于直面官方文档进行学习。Pyecharts是一个功能强大的库，能制作出许多精美的图表，用于数据可视化大屏展示很方便，但是有一些细节还不够完善。

​	python包含很多功能强大的模块和第三库，有时间一定要多接触接触。

## 3.4 性能改进心得体会

- 体会

​		看官方文档能够解决很多的问题，例如对于一个模块使用方式的改进。但是自身对于性能测试方面的知识还不是特别了解，以后还需要多多练习。通过这次软工作业，我还认识到了进行单元测试的方法、进行自动化单元测试的方法，不过用起来仍然不是特别熟练，在后续的作业还需要加强练习。
