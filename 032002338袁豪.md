<https://github.com/yh7383/032002338>
# 一、PSP表格
## 1.1在开始实现程序之前，在附录提供的PSP表格记录下你估计将在程序的各个模块的开发上耗费的时间。
| 程序模块 | 预估耗时 |
|--|--|
|计划|1小时|
|-估计这个任务多少时间|1小时|
|开发|27小时|
|-需求分析|4小时|
|-生成设计文档|0.5小时|
|-设计复审|0.5小时|
|-代码规范|0小时|
|-具体设计|1小时|
|-具体编码|10小时|
|-代码复审|5小时|
|-测试|6小时|
|报告|3小时|
|-测试报告|1小时|
|-计算工作量|1小时|
|-事后总结|1小时|
|合计|31小时|
## 1.2在你实现完程序之后，在附录提供的PSP表格记录下你在程序的各个模块上实际花费的时间。
| 程序模块 | 实际耗时 |
|--|--|
|计划|1小时|
|-估计这个任务多少时间|1小时|
|开发|30小时|
|-需求分析|3小时|
|-生成设计文档|0.5小时|
|-设计复审|0.5小时|
|-代码规范|0小时|
|-具体设计|2小时|
|-具体编码|13小时|
|-代码复审|5小时|
|-测试|6小时|
|报告|3小时|
|-测试报告|1小时|
|-计算工作量|1小时|
|-事后总结|1小时|
|合计|34小时|
---
# 二、任务要求的实现
## 2.1项目设计与技术栈。从阅读完题目到完成作业，这一次的任务被你拆分成了几个环节？你分别通过什么渠道、使用什么方式方法完成了各个环节？列出你完成本次任务所使用的技术栈。
1. **爬取卫健委每日疫情报告网站的url**
   1. 渠道：先通过爬虫相应的教程，对爬虫的过程有一定的了解之后。通过百度和必应搜索信息，查找到所需要的技术栈，及函数调用等信息。
   2. 技术栈：python。
2. **爬取卫健委每日疫情报告网站的具体文本信息**
   1. 渠道：通过教程网站和搜索引擎，了解到X-path和Beautiful Soup语法规定及用法。
   2. 技术栈：python。
3. **对卫健委每日疫情报告网站文本信息进行提取**
   1. 渠道：通过教程网站对正则表达式的学习，再通过正则表达式测试网站对自己的正则表达式进行测试；之后通过对数据结构的分析将省份与人数分开，存入dict中。
   2. 技术栈：python。
4. **对提取的信息进行可视化处理**
   1. 渠道：渠道通过python教程了解到python可视化工具，再通过百度和必应的搜索，查找到逍遥可视化的代码模板，进行修改得到自己的可视化结果。
   2. 技术栈：python、html。
## 2.2爬虫与数据处理。说明业务逻辑，简述代码的设计过程（例如可介绍有几个类，几个函数，他们之间的关系），并对关键的函数或算法进行说明。
- **爬虫**
    1. **获取网站html页面**
        - 通过request库爬取html
            ```python
            #python
            def get_html1(self, url):
            headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36 Edg/105.0.1343.27',
                        'Referer' : 'http://www.nhc.gov.cn/xcs/xxgzbd/gzbd_index.shtml',
                        'Cookie' : 'yfx_c_g_u_id_10006654=_ck22090720162712618380357341525; yfx_mr_10006654=%3A%3Amarket_type_free_search%3A%3A%3A%3Abaidu%3A%3A%3A%3A%3A%3A%3A%3Awww.baidu.com%3A%3A%3A%3Apmf_from_free_search; yfx_mr_f_10006654=%3A%3Amarket_type_free_search%3A%3A%3A%3Abaidu%3A%3A%3A%3A%3A%3A%3A%3Awww.baidu.com%3A%3A%3A%3Apmf_from_free_search; yfx_key_10006654=; sVoELocvxVW0S=5sxvsYLD5ZFBzdRzhbC4AqoRB9l7h6ao5z21THksRXuqKTHBC0JbfhBRI4iRxlAGbjsIN5XjjOwTHRdjc6aUZWa; _gscu_2059686908=62877258pc2qxl10; security_session_verify=1b6ba012eeceacfed9b9e3bed608dabb; yfx_f_l_v_t_10006654=f_t_1662552987258__r_t_1663041656267__v_t_1663044006166__r_c_7; insert_cookie=96816998; sVoELocvxVW0T=53nqtWKWGcOEqqqDkJ1IsmG9Jc_5QnfMibYrBD1tYa3gKJgZI9ADLYGNDeitkjnyU6nEB7Oxz6Lyh3lsGokfxBt0HfScHDonq2vX2p6kDffmkffzkxho1dgP2qrsTVz9mk4nbqTO8oC8nXMr.J2vYOVC0t2BnGZ1EoAY5L_a.sbphQx4.Xx2PyznMDWWEcfsr0JxePjZx.Jwpfi91Ni9Vzp8dR_lNFXbbKBxj7qEXc_Os00w2gBUDLNhLgjFoWnsbgn55M8cco1g1AcyUiPcpmQRs9ecz874sf4bRaLvuR0g6hvt7JF_bkC5.PCvcxo9RA'
                    }#配置headers，使得能够进入网页
            req = request.Request(url=url, headers=headers)#发送访问请求
            html = request.urlopen(req).read().decode('utf-8')#得到html页面element，并设置编码方式
            return html
            ```
            有的同学配置一个UA代理可以跑完所有数据，有的同学需要配置Referer和Cookie可以跑完数据。但是有的同学~~超级倒霉蛋~~（是我）会遇到配置headers之后没办法或者频繁进不了网页，运行不了程序，报412错误，被卫健委拒之门外。下面给出三种解决报412办法。
        - 通过selenium库的webdriver爬取html\
            此方法除了需要安装相应的库之外还需要给浏览器下载相应的驱动器，具体过程可以[点击这里]().
            具体代码如下：
            ```python
            #python
            def get_html(url):
                while 1:
                    options=webdriver.FirefoxOptions()
                    options.add_argument('--headless')#无界面浏览
                    options.add_argument('blink-settings=imagesEnabled=false')
                    browser = webdriver.Firefox(options=options)
                    #声明模拟浏览器，并根据配置进行设置
                    browser.get(url)
                    #输入网址，并访问页面（模拟浏览器打开）
                    print(url)
                    sleep(1)
                    n_page_text = browser.page_source 
                    #页面的html的element信息         
                    soup = BeautifulSoup(n_page_text, 'html.parser')
                    list_name = soup.select('.list > div')
                    #通过BS4匹配想要的信息（段落的文本信息）
                    print(len(list_name))
                    if len(list_name)==0: #无信息则访问失败
                        browser.quit()
                        pass
                    else: #有信息则访问成功
                        browser.quit()
                        break 
                return list_name
            ```
            通过selenium库的driver可以实现模拟打开浏览器，然后打开页面从而避免报412错误。但是会存在两个问题：
            - 访问页面速度太慢，一个页面需要模拟浏览器的打开。
            - 会访问到无效页面（信息加密），这是必须关闭浏览器重新打开，让本就慢的速度雪上加霜。


            总之，经过经验可得，用selenium的库跑完所有页面近乎需要一晚上的时间。为此，给出下一个方法。
        - 通过pyppeteer库的launch爬取html
            通过调用launch实现模拟打开对应的谷歌浏览器chromium，实现模拟打开页面与selenium相似，pyppeteer库和chromium下载可以[点击这里]()。\
            打开声明browser：
            ```python
            #python
            #在主函数调用先打开，避免重复打开浏览器
            start_parm = {
                    # 启动chrome的路径
                    "executablePath":r"C:\Users\sbhh\AppData\Local\Chromium\Application\chrome.exe",
                    # 关闭无头浏览器 默认是无头启动的
                    "headless": True,
                    'dumpio':True,
                    'autoClose':True
                }
            # 创建浏览器对象
            browser = await launch (start_parm)
            ```
            用声明的browser打开页面并返回html：
            ```python
            #python
            def get_html(browser,url):
                while 1:
                    page = await browser.newPage()  
                    #浏览器打开页面
                    flag=await page.goto(url)  # 页面跳转
                    n_page_text = await page.content()  # 页面内容      
                    soup = BeautifulSoup(n_page_text, 'html.parser')
                    list_name = soup.select('.list > div')
                    #通过BS4匹配想要的信息（段落的文本信息）
                    print(len(list_name))
                    if len(list_name)==0: #无信息则访问失败
                        browser.quit()
                        pass
                    else: #有信息则访问成功
                        browser.quit()
                        break 
                return list_name    
            ```
            pyppeteer的优势：
            - 能够打开页面，避免重复打开浏览器，提高速度。
            - 能够使用异步，是浏览器打开的速度大大加快。

            刚才说的两种方法都是模拟打开浏览器，有说有一，不仅对配置有要求，要下载驱动器或者新浏览器，而且速度方面还是有所欠缺。接下来介绍一种更有效的办法，但是比较看运气。
        - 通过抄去能够快速爬虫同学的hearders\
            有的同学一个UA+Referer+Cookie组成的headers能够把所有的数据都跑出来，这个时候我们可以找一找这些同学，请求~~（乞求）~~他们把他们的headers分享给我们，我们就可以向他们一样快乐的爬虫啦！\
            这是我舍友赵先生的无敌haeders：
            ```python
            #python
            headers =  {
                    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36 Edg/105.0.1343.27',
                    'Referer' : 'http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml',
                    'Cookie': 'yfx_c_g_u_id_10006654=_ck22091015454118598777723981237; sVoELocvxVW0S=5DSg3AJl0Eh2sJMRS0xBrJW.qk641GDuqy0w_08ci5Ki5RdtDaxR5X6SCxJU2lUT2JAIRrk5bgMicvJNmcncBLA; insert_cookie=91349450; _gscu_2059686908=62796040qrs9mc90; _gscbrs_2059686908=1; yfx_f_l_v_t_10006654=f_t_1662795941823__r_t_1662866266927__v_t_1662876292876__r_c_1; security_session_verify=ce983d3c2eeacced2208f3cd4fc9e426; sVoELocvxVW0T=53STdBCWwGhlqqqDkt0rQtG6hvNNyjExrIU3bZflvEnBgUiy0rpSXvPhIFquw.HpHkcLWwzYGQ6eEHopJXmPiqMH7P7nL09.CikMkdGvjHqdEVKzpKiFU60R530mcGomp82jqaQL3a.guQHYmHnb8EGk9nyNGpDJtpIqUlaBaWa4wQtfM3rt96HTunysuql4i5fPS4RFVke8_drqEBZUr9U8w4Ft1xHFp8bgYNqx9XEZOpYCBVMHCFEDNKT2fB88YtHP1XwYLY6rySFRX0sxoeyl3y8i5WhmKypGqBUkoSGuy.tTEACdvFM9IJQglti.iCaRYfu4_EvP28kJ69CTWulvhNH1OSwC6nG1zr6BzkvTG'
                }
            ```
- **数据处理**\
    1. **获取文本信息和得到省份与人数的dict**\
        这里建议直接获取整个文本的所有段落信息，再进行正则拆分到新增确诊病例和新增无症状感染者，不要先通过筛选出每个段落，再正则，一定要对所有段落正则。因为会存在日期不同，段落设置不一样，从而造成得到错误信息。
        - 用BS4得到整个文本信息
            ```python
            #python
            n_page_text=get_html1(url)        
                #得到html信息       
                soup = BeautifulSoup(n_page_text, 'html.parser')
                #通过html信息生成soup
                list_name = soup.select('.list > div')
                #div标签后所有文本信息
            ```
        - 用文本信息得到新增确诊省份和对应人数\
            ```python
            #python
            def get_xin(list_name):
                for item in list_name:
                    pattern = re.compile('新疆生产建设兵团报告新增确诊病例.*?本土病例(.*?)）') #pattern
                    xx = pattern.findall(str(item.text))
                    #通过正则匹配整个文本信息
                    data = str(xx) #转换成字符串格式
                    for i in range(0,len(data)):
                        if '1'<=data[i] and data[i]<='9' and not( '1'<=data[i-1] and data[i-1]<='9'):
                            #识别到第一个数字
                            addnum1(data,i)
                            #通过第一个数字向前匹配省份
                    addnum3(data)
                    #均在同一个省份的情况
            #正常文本下，获得新增确症省份和人数并用dict存储
            def addnum1(data, i):
                num=0
                for j in range(i,len(data)):
                    #向后扫描数字
                    if('0' <= data[j] and data[j] <= '9'):
                        num*=10
                        num+=int(data[j])
                    else:
                        break
                for j in range(i-1,0,-1):
                    #向前扫描省份
                    if data[j]=='，' or data[j]=='；' or data[j]=='\'' or data[j]=='（':
                        exc1[data[j+1:i]]=num
                        #将数字加入对应的省份
                       break
            #在只有一个省有新增的情况
            def addnum3(data):
                for j in range(0,len(data)):
                    if data[j]=='均':
                    #识别到“均”后开始搜索
                        num=0
                        for k in range(0,j):
                        #向前搜索数字
                            if data[k]>='0' and data[k]<='9':
                                num*=10
                                num+=int(data[k])
                        for k in range(j+3,j+6):
                        #向后搜索省份
                            exc1[data[j+2:k]]=num
            ```
        - 用文本信息得到新增无症状感染者省份和对应人数\

            ```python
            #python
            def get_wu(list_name):
                for item in list_name:
                    pattern = re.compile('新疆生产建设兵团报告新增无症状感染者.*?本土(.*?)）') #pattern
                    xx = pattern.findall(str(item.text))
                    #用正则匹配到本土新增的省份文段
                    data = str(xx)
                    for i in range(0,len(data)):
                        if '1'<=data[i] and data[i]<='9' and not( '1'<=data[i-1] and data[i-1]<='9'):
                            addnum2(data,i)
                            #锁定新增无症状感染者的省份和人数             
                    addnum4(data) #在只有一个省份出现的特殊情况
            def addnum2(data, i):
                num=0
                for j in range(i,len(data)):
                #识别数字
                    if('0' <= data[j] and data[j] <= '9'):
                        num*=10
                        num+=int(data[j])
                    else:
                        break
                for j in range(i-1,0,-1):
                #识别省份
                    if data[j]=='，' or data[j]=='；' or data[j]=='\'' or data[j]=='（':
                        exc2[data[j+1:i]]=num
                        break
            def addnum4(data):
                #只有一个省份无症状感染者的特殊情况
                for j in range(0,len(data)):
                    if data[j]=='均':
                        num=0
                        for k in range(0,j):
                        #向前搜数字
                            if data[k]>='0' and data[k]<='9':
                                num*=10
                                num+=int(data[k])
                        for k in range(j+3,j+6):
                        #向后搜省份
                            exc1[data[j+2:k]]=num
            ```
        - 用文本信息得到港澳台的情况\

            ```python
            #python
            def get_tai(list_name):
                for item in list_name:
                    pattern = re.compile('.*?香港特别行政区(.*?)例.*?澳门特别行政区(.*?)例.*?台湾地区(.*?)例（') #pattern
                    xx = pattern.findall(str(item.text))
                    data = str(xx)
                    #正则匹配到数字
                    j=1
                    for i in range(1,len(data)):#扫描字符串
                        if '0' <= data[i] and data[i] <= '9' and data[i-1] == '\'':#开头是数字
                            num=0
                            #识别数字
                            for k in range(i,len(data)):
                                if '0' <= data[k] and data[k] <= '9':
                                    num*=10
                                    num+=int(data[k])
                                else:
                                    break
                            #将数字加入到省份信息的dict
                            if j==1:
                                exc1['香港']=num
                            elif j==2:
                                exc1['澳门']=num
                            else:
                                exc1['台湾']=num
                            j+=1
                            i=k
            ```
    2. **存储数据**
        - 每日信息的存储\
            调用的函数将省份与其对应的人数存储到dict，每日用dict临时存储，再每日所有情况都扫描完之后，转换为pandas库中Series（各个省份作为index）。
        - 所有信息的存储\
            所有信息用pandas库中的DataFrame（df）来存储，省份作为index，日期作为columns。
        - 所有信息和每日信息对接组合\
            将Series以当天日期作为columns加入DataFrame中，然后将DataFrame存入excel中，防止程序出现问题而丢失，代码如下：
            ```python
            #python
            def store_data(webs,ii):
                df1[webs[ii*2+1]]=pd.Series(exc1)
                df2[webs[ii*2+1]]=pd.Series(exc2)
                #将dict转化为Series，以日期做columns存入DataFrame中
                print(df1)
                print(df2)
                #查看情况
                for key1,key2 in zip(exc1,exc2):
                    #重置dict
                    exc1[key1]=0
                    exc2[key2]=0   
                #数据导入excel
                writer = pd.ExcelWriter('1.xlsx')
                df1.to_excel(writer)
                writer.save()
                writer = pd.ExcelWriter('2.xlsx')
                df2.to_excel(writer)
                writer.save()
            ```
## 2.3数据统计接口部分的性能改进。记录在数据统计接口的性能上所花费的时间，描述你改进的思路，并展示一张性能分析图（例如可通过VS 2019/JProfiler的性能分析工具自动生成），并展示你程序中消耗最大的函数。
- 爬取数据用selenium的库进行操作,i安林效率低下的问题,我们可以用可以一直跑数据的headers进行配置,用request库进行爬取.
- 有的数据或者结构是可以避免或者不去使用,能够提高我们的软件性能
![image](https://img2022.cnblogs.com/blog/2288880/202209/2288880-20220919235136837-1409234488.png)


## 2.3每日热点的实现思路。简要介绍实现该功能的算法原理，可给出必要的步骤流程图、数学公式推导和核心代码实现，并简要谈谈所采用算法的优缺点与可能的改进方案。
1. **算法原理**\
   每天的热点通过，查询前一天的疫情状况，今天与昨天的对比，从而找到当天的热点。分为某地出现新疫情，某地疫情清零，某地疫情爆发等等，根据数据之间的差值判断，哪些地方出现了热点。
2. **优缺点**
   - 优点
     - 实现简单
     - 快速掌握每天疫情的基本情况
   - 缺点
     - 数据采取过于单一，只参考前一天
     - 内容过于单一
3. **代码**
   ```python
   #python
   def hot(webs,ii):
    list_sheng=list(df1.index)
    print(webs[ii*2+1])#热点日期
    print("的热点：\n")
    for sheng in list_sheng:
        if df1[webs[ii*2+1]][sheng]!=0:#这天有疫情
            print(sheng)
            if df1[webs[ii*2+3]][sheng]==0:#前一天无疫情
                print('出现疫情\n')
            elif df1[webs[ii*2+3]][sheng]>=df1[webs[ii*2+1]][sheng]:#疫情增加
                print('疫情有所好转\n')
            else:#疫情减少
                print('疫情更加严重\n')
        else:#疫情结束
            if df1[webs[ii*2+3]][sheng]>0:
                print(sheng)
                print('疫情清零\n') 
    ```
4. **改进方案**\
   可以通过可视化的折线图判断某地疫情处于的状态，是增加还是减少，都可以有比较清晰的了解，也可以结合前几日的信息。
## 2.4数据可视化界面的展示。在博客中介绍数据可视化界面的组件和设计的思路。
1. **可视化设计思路**\
   可视化的目的在于让我快速得到我们想要的信息,通常我们希望看到每天疫情的具体情况和某个省份再时间维度上的感染人数，即空间维度和时间维度两个维度的角度来设计可视化页面。所以我导入每天疫情状况的地图页面，和省份在时间维度上的疫情折线图。
2. **某天全国疫情情况的可视图**\
   ![image](https://img2022.cnblogs.com/blog/2288880/202209/2288880-20220919234546601-2077175682.png)
3. **某省近两年的疫情变化的可视图**\
   ![image](https://img2022.cnblogs.com/blog/2288880/202209/2288880-20220919234653255-1516357864.png)
   
# 三、心得体会
1. 提升了学习的能力。在几天中，学习了python的基本数据结构和基本用法，还学习了python的爬虫，pyhthon的几种库的基本用法，都是在有限的时间内完成的。无疑这些实践无疑都提升了我快速学习的能力。
2. 提升了快速查找信息的能力。有许多的内容不需要我们有深入的了解，只需要我们会用即可，这时候就需要我们能够快速查找到信息，截取到对我们有用的信息。往往查找的内容很多是我们不需要的，这个时候就需要我们快速进行有效筛选。
3. 提升了我解决问题的能力。在本次任务中，我遇到python爬虫无法进入卫健委的问题，用正常爬虫的思路，会频繁报告412错误。这个时候只能不断去查找资料寻找到我想要的信息，查找解决问题的方法，从各个角度去描述寻找我遇到的问题。从而我找到了运用了selenium库，pyppeteer库，用别人的headers等等方法。知道了这些方法的实现原理，能够通过实践分析出这些数据的优缺点。
4. 提高了我的知识面，几天的学习丰富了我的知识储备。通过爬虫也让我感受到了计算机的魅力和强大，对计算机有新的认知。
        
