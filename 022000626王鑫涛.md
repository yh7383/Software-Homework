**github仓库：**https://github.com/wangxintao2002/022000626.git



# 一、PSP表格

| **PSP2.1**                               | **Personal Software             Process Stages** | 预计耗时 （分钟） | 实际耗时（分钟） |
| ---------------------------------------- | ------------------------------------------------ | ----------------- | ---------------- |
| Planning                                 | 计划                                             | 80                | 100              |
| · Estimate                               | · 估计这个任务需要多少时间                       | 1200              | 1000             |
| Development                              | 开发                                             | 720               | 600              |
| · Analysis                               | · 需求分析 (包括学习新技术)                      | 300               | 300              |
| · Design Spec                            | · 生成设计文档                                   | 60                | 60               |
| · Design Review                          | · 设计复审                                       | 60                | 60               |
| · Coding Standard                        | · 代码规范 (为目前的开发制定合适的规范)          | 60                | 60               |
| · Design                                 | · 具体设计                                       | 60                | 60               |
| · Coding                                 | · 具体编码                                       | 60                | 60               |
| · Code Review                            | · 代码复审                                       | 60                | 60               |
| · Test                                   | · 测试（自我测试，修改代码，提交修改）           | 60                | 60               |
| Reporting                                | 报告                                             | 100               | 100              |
| · Test Repor                             | · 测试报告                                       | 60                | 60               |
| · Size Measurement                       | · 计算工作量                                     | 60                | 60               |
| ·  Postmortem & Process Improvement Plan | · 事后总结, 并提出过程改进计划                   | 60                | 60               |
|                                          | · 合计                                           | 5000              | 4700             |

# 二、任务要求的实现



## 2.1 项目与技术栈

### 项目流程分析

- 编写爬虫程序爬取数据
- 对爬取到的数据进行处理
- 将处理后的程序写入excel表格并生成图表
- 将数据以可视化大屏的形式实现

### 各个流程实现

- 网页内容爬取：Python中的**pyppeteer**模块、**xpath**数据解析
- 文本中提取数据：正则表达式，os模块处理文件
- 数据存储到Excel中：**openpyxl**模块
- 数据每日热点分析
- 数据可视化：openpyxl模块、第三方库pyecharts

### **技术栈**

**python**（**pyppeteer**模块、**asyncio**模块、**os**模块、**re**模块、**openpyxl**模块）



## 2.2 爬虫与数据处理

### **需求分析**

统计中国大陆每日本土新增确诊病例以及新增新增无症状感染病例。

### 实现过程

#### 编写crawler.py实现爬虫

- 定义get_page_url函数，返回卫健委疫情通报41页的url

```python
def get_page_url():
    url_list = []

    #遍历41页
    for page in range(1, 42):
        if page == 1:
            url_list.append('http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml')
        else:
            page_url = 'http://www.nhc.gov.cn/xcs/yqtb/list_gzbd_' + str(page) +'.shtml'
            url_list.append(page_url)
    return url_list
```

- pyppeteer_fetchContent：爬取给定url的原代码

```python
async def pyppeteer_fetch_content(url):
    #以headless模式打开浏览器，将浏览器处理标准输出导入process.stdout，并设置自动关闭

    browser = await launch({'headless': True, 'dumpio': True, 'autoClose': True})

    page = await browser.newPage()
    # 绕过浏览器检测
    await page.evaluateOnNewDocument('() =>{ Object.defineProperties(navigator,'
                                     '{ webdriver:{ get: () => false } }) }')

    #设置UA，模拟人手动打开页面
    await page.setUserAgent(
        'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Mobile Safari/537.36 Edg/105.0.1343.27')
    await page.goto(url)

    #等待页面转到一个新链接或者是刷新
    await asyncio.wait([page.waitForNavigation()])

    #获取爬下来的内容
    str = await page.content()
    await browser.close()
    return str
```



- get_link_url：根据参数html，用xpath解析出每一页24天数据的url

```python
def get_link_url(html):
    day_urls = []
    html_tree = etree.HTML(html)

    #解析出url位置
    li_list = html_tree.xpath("/html/body/div[3]/div[2]/ul/li")
    for li in li_list:
        day_url = li.xpath('./a/@href')
        day_url = "http://www.nhc.gov.cn" + "".join(day_url[0])
        day_urls.append(day_url)
    return day_urls
```

- get_content：获取每一天的数据

```python
def get_content(html):
    html_tree = etree.HTML(html)
    text = html_tree.xpath('/html/body/div[3]/div[2]/div[3]//text()')
    day_text = "".join(text)
    return day_text
```

- work：核心函数,爬取并把数据存入本地

```python
def work(url):
    s = get_page_souce(url)
    filenames = get_files(s)
    print(filenames)
    index = 0
    links = get_link_url(s)
    for link in links:
        html = get_page_souce(link)
        content = get_content(html)
        print(filenames[index] + "爬取成功")
        save_file('D:/数据/', filenames[index], content)
        index = index + 1
```

#### process.py 处理数据

- process_new：处理每天新增无症状或者新增确诊，并存入字典

```python
def process_new(filenames, basePath, str):
    for filename in filenames:
        # 截取文件名前10个字符，得到文件的日期
        date = filename[0:10]

        d = dict()
        # 如果此时为判断新增确诊，则以日期为key在全局变量里建一个字典
        if str == '新增确诊':
            alldata[date] = dict()
        # 打开对应文件
        file = open(basePath + "\\" + filename, 'r', encoding='utf-8')
        content = file.read()
        # 编译正则表达式，得到文本中有关新增确诊或新增无症状的信息
        reg = r'本土病例.*?）' if str == '新增确诊' else r'本土\d.*）(?=。\n当日解除|；当日解除|；当日无)'
        pattern = re.compile(reg)
        result = re.search(pattern, content)

        #
        if result:
            # 求出当日本土新增总病例
            result = result.group()
            start = 4 if str == '新增确诊' else 2
            end = "例（" if str == '新增确诊' else "例"
            d[str] = int(result[start:result.find(end)])
            # 求出各省份新增病例
            provinces = result
            # 把括号里的内容提取出来，便于数据处理
            provinces = provinces[provinces.index('（') + 1:provinces.index("）")]

            # 处理数据，把provinces里各省份与其新增确诊或新增无症状病例
            # 作为key，value存入alldata[date][str]
            process_reg(provinces, d, str)

        else:
            #找不到，说明当日新增确诊或新增无症状为零
            d[str] = 0
        alldata[date][str] = dict()
        alldata[date][str].update(d)
```

- process_reg：用正则表达式提取数据

```python
def process_reg(province, d, str):
    if province.find('，') == -1:
        for name in provinceList:
            if name in province:
                d[name] = int(d[str])
                return
    else:
        if province.find("均") == 0 or province.find("在") == 0:
            for name in provinceList:
                if name in province:
                    d[name] = int(d[str])
                    return
        for name in provinceList:
            pattern = re.compile(r"%s[0-9]*例" % name)
            data = re.search(pattern, province)
            if data:
                data = data.group()
                d[name] = int(data[data.find(name) + len(name):data.find('例')])
```

- pipe_into_excel：将数据导出到excel

```python
def pipe_into_excel(date):
    wb = Workbook()
    sheet1 = wb.create_sheet(index=1, title="新增确诊")
    sheet2 = wb.create_sheet(index=2, title="新增无症状")
    i = 1

    for key, value in alldata[date]['新增确诊'].items():
        sheet1.cell(i, 1, value=key)
        sheet1.cell(i, 2, value=value)
        i = i + 1
    i = 1
    for key, value in alldata[date]['新增无症状'].items():
        sheet2.cell(i, 1, value=key)
        sheet2.cell(i, 2, value=value)
        i = i + 1

    wb.save('D:\\excel\\' + date + '.xlsx')
```

**最后写一个run.py来执行**

### 关键函数

**process_reg(province, d, str) **：

**province**：各省份的新增确诊或新增无症状通告

**d**：存储处理后的数据

**str**：处理类型，传入 ‘ 新增确诊 ‘ 则对各省新增确诊病例进行处理，传入’ 新增无症状 ‘ 则对各省新增无症状病例进行处理

**函数判断通告里有无逗号：**

- **无逗号**：说明当天所有确诊病例或无症状病例均来自一个省，可直接遍历各省份名，如果省份名出现在通告里，说明当天的病例出现在该省份，以省份名为key，病例数为value存入字典
- **有逗号**：分为两种情况
  - 如果第一个字是“均”或“在”，说明当天所有确诊病例或无症状病例均来自一个省（例如：均在吉林省，吉林市）
  - 否则，遍历所有省份名，编译正则表达式 （模式为：<省份名><任意多个数字>例，找到所有匹配项，以’省份名‘：’病例数‘ 形式存入字典

```python
def process_reg(province, d, str):
    #如果在province里找不到逗号，说明当日确诊均在某一个省
    #直接遍历provinceList里的省份，如果某个省份出现在province里
    #就把该省份作为key，当日新增确诊作为value存入字典
    if province.find('，') == -1:
        for name in provinceList:
            if name in province:
                d[name] = int(d[str])
                return
    else:
        #如果找到逗号，且第一个字是“在”或者“均”，则说明也是上面一种情况
        #处理方式同上
        if province.find("均") == 0 or province.find("在") == 0:
            for name in provinceList:
                if name in province:
                    d[name] = int(d[str])
                    return
        #如果不是，则说明有多个省份有新增确诊，遍历省份列表
        for name in provinceList:
            #编译正则表达式，匹配 <省份名><任意多个数字>例 这种模式的数据
            pattern = re.compile(r"%s[0-9]*例" % name)
            data = re.search(pattern, province)
            #如果找到该省份的数据
            if data:
                data = data.group()
                #获取数据后面的病例数并存入字典
                d[name] = int(data[data.find(name) + len(name):data.find('例')])
```



## 2.3 数据统计接口部分的性能改进

一开始用未优化的程序运行，爬取数据就花了2个小时

**改进思路：**利用多线程爬虫，可以大幅度减少运行时间



改进后的运行时间：

![QQ图片20220917143728.png](https://s2.loli.net/2022/09/19/y6YPsfR9ZipVnAq.png)



## 2.4 每日热点的实现思路

利用LSTM(长短时记忆神经网络)

## 2.5 数据可视化界面的展示



# 三、心得体会

写这个项目前，我没接触过python和爬虫，所以项目前期还是比较困难的，花了一个下午的时间在b站看教程，跟着视频敲代码，只能爬爬简单的网页。后来尝试用requests模块去爬卫健委数据，但是一直返回412状态码，我尝试搭一个代理池，然而免费的代理IP根本不好用...在网上找遍方法，最后决定用pyppeteer，但网上没有教程，所以只好去官网看文档。研究了一下午，总算成功爬取到了数据。

这次作业我受益匪浅，从不会python到现在能用python写个简单的爬虫程序，在处理数据的过程中我学到了如何用正则表达式来提取数据，

